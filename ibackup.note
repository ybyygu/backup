#+TITLE: 系统数据自动备份
#+SETUPFILE: ~/Notes/common.org
#+TODO: TODO | DONE(@)
#+PROPERTY: mkdirp yes
#+PROPERTY: padline no

#+name: UPDATED
#+BEGIN_SRC rst
Updated: 2015-08-09 10:06
#+END_SRC


#+name: VERSION
#+begin_src rst
v0.1.4
#+end_src

* 摘要
  * 将isync-data下的数据 *实时* 同步至isync-backup下
  * 在isync-snapshots下做快照, 保存版本信息
  * 按指数衰减算法清理isync-snapshots下的快照
* [3/3] Checklist
  1. [X] ifmain version, time-stamp...
  2. [X] btrfs
  3. [X] lsyncd.lua
* [3/3] Memo
** [2015-08-09 Sun] 使用flock文件锁, 解决lsyncd同时运行的问题 参考: [[http://www.kfirlavi.com/blog/2012/11/06/elegant-locking-of-bash-program/][URL]]
            
** DONE [2015-04-18 Sat] isync-data不应该放在etc这个目录下, 可能导致无穷递归.
   - State "DONE"       from "TODO"       [2015-04-26 Sun 20:32] \\
     似乎没问题的.
   放在/backup下要好很多.
** DONE [#B] [2013-07-23 Tue] lsyncd 2.1新版 无法使用之前写的config文件, 需要升级 :Action:
   - State "DONE"       from "TODO"       [2015-04-18 Sat 11:43]
** [2012-06-29 Fri]: 2.0.7版更新. 以后有空了注意与rsyncssh.lua同步

   需要处理rsyncExitCodes之类的东西. rsyncExitCodes = default.rsyncExitCodes,

** DONE [2012-06-17 Sun]: 使用牛顿冷却公式来处理文件备份的问题 [[http://songshuhui.net/archives/67391][URL]]
   - State "DONE"       from "TODO"       [2015-04-19 Sun 13:32] \\
     已应用.
** [2011-09-05 Mon]: 注意exclude文件里不能有空行, 否则所有文件都会被排除掉.

** [2011-05-19 Thu] update to lsyncd-2.0
   新版的优点:
   * 可以处理move事件, 不用像以前那样进行"delete here >> copy there"的
     操作.
   * 配置文件使用lua语法.

* 架构规划
  1. 数据目录: source_directories
     1) 基于目录+软链接的方式管理起来更方便直观些.
     2) 源目录通过软链接到source_rootdir(isync-data)下
        : isync-data/note --> ~/Notes
        其中~/Notes是真正要备份的目录: source_directory

  2. 目标目录(备份数据至..): backup_directory
     - backup_rootdir(/btrfs/backup): 用于数据备份的根目录, 所有源目录都将同步其下的
       子目录
     - backup_directory: 真正用于数据备份的目录
       如1.2中的例子:
       : backup_directory = /btrfs/backup/note
     - 如果backup_rootdir是btrfs文件系统的一个子卷(subvolume), 则可基
       于基snapshot功能, 做版本备份(TimeMachine).
     - snapshot_rootdir (/btrfs/TimeMachine): 用于在其下存储btrfs子卷
       (/btrfs/backup)快照. 应为btrfs文件系统下一普通目录.
     - 基于日期的快照命名: snapshot_rootdir/day/time
       : /btrfs/TimeMachine/2011-09-04/22:13
     - 快照清理
       - 需要自动清理旧的备份(指数衰减)
* BTRFS 参见[[file:~/Notes/cmdline.note::*BTRFS][cmdline:btrfs]]
** 技术要点
   1. BTRFS 做增量备份: [[https://btrfs.wiki.kernel.org/index.php/Incremental_Backup][incremental backup]]
      "-o" 做只读版本
   2. 启用lzo压缩, 节省空间.
** 操作步骤
   1. 格式化btrfs分区. 这里用lvcreate 生成名为/dev/store/btrfs 的虚拟卷
   2. 将虚拟卷格式化为btrfs
      : mkfs.btrfs /dev/store/btrfs
   3. 修改/etc/fstab
      : /dev/store/btrfs  /btrfs  btrfs   defaults,compress=lzo           0       2
   4. 创建/btrfs/backup子卷, 方便快照操作.
      : btrfs subvolume create /btrfs/backup
   5. 创建/btrfs/TimeMachine目录, 用于存放快照
      : mkdir /btrfs/TimeMachine
   6. 创造只读快照
      #+name: btrfs-take-snapshot
      #+begin_src sh
        mkdir -p /btrfs/TimeMachine/2015-04-22
        btrfs subvolume snapshot -r /btrfs/backup /btrfs/TimeMachine/2015-04-22/15:54
      #+end_src

      #+RESULTS: btrfs-take-snapshot
      : Create a readonly snapshot of '/btrfs/backup' in '/btrfs/TimeMachine/2015-04-22/15:54'

   7. 删除快照(需要root权限):
      #+name: btrfs-delete-snapshot
      #+begin_src sh
        sudo btrfs subvolume delete /btrfs/TimeMachine/2015-04-22/15:54
        rmdir /btrfs/TimeMachine/2015-04-22
      #+end_src

      #+RESULTS: btrfs-delete-snapshot
      : Delete subvolume (no-commit): '/btrfs/TimeMachine/2015-04-22/15:54'

* logging
** 相关资料: [[http://victorlin.me/posts/2012/08/26/good-logging-practice-in-python][Good logging practice in Python]]
** guts中的设置(很多问题)
   #+NAME: bad-logger
   #+BEGIN_SRC python :results output
     import logging
     import sys

     LogThemeNull = logging.Formatter(
         " %(message)s")
     LogThemeSimple = logging.Formatter(
         "[%(levelname)s] %(message)s")
     LogThemeClean = logging.Formatter(
         "[%(name)s|%(lineno)04d %(levelname)s] %(message)s")
     LogThemeClean2 = logging.Formatter(
         "[%(name)s|%(lineno)04d %(levelname)5s %(asctime)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
     LogThemeFull = logging.Formatter(
         "\n%(asctime)s - %(name)s.%(funcName)s, line %(lineno)d\n" \
         + "-"*80 + "\n [%(levelname)s] %(message)s",
         datefmt="%Y-%m-%d %H:%M:%S")

     class ConsoleHandler(logging.StreamHandler):
         """
         log messages to console screen
         >> direct logging.error and above to stderr, others to stdout
         """
         def __init__(self, *args, **kwargs):
             logging.StreamHandler.__init__(self, *args, **kwargs)

         def emit(self, record):
             if record.levelno >= logging.WARN:
                 self.stream = sys.stderr
             else:
                 self.stream = sys.stdout
             logging.StreamHandler.emit(self, record)

     def setup_logger(name=None, theme="null", logfile=None, verbose=True, root=False):
         """ if name is empty, you get the root logger
         @root: root logger of not
         """

         Themes = {"clean":LogThemeClean,
                   "clean2": LogThemeClean2,
                   "simple":LogThemeSimple,
                   "null":LogThemeNull,
                   "full":LogThemeFull
                   }

         # make levelname shorter
         if theme in ("clean", "simple"):
             logging.addLevelName(logging.DEBUG, "D")
             logging.addLevelName(logging.INFO, "I")
             logging.addLevelName(logging.WARN, "W")
             logging.addLevelName(logging.ERROR, "E")

         if name and not root:
             logger = logging.getLogger(name)
         else:
             logger = logging.getLogger()

         logger.setLevel(logging.DEBUG)

         # keep console screen clean
         console_hdlr = ConsoleHandler()
         if verbose:
             console_hdlr.setLevel(logging.DEBUG)
         else:
             console_hdlr.setLevel(logging.INFO)
         theme = Themes[theme.lower()]
         console_hdlr.setFormatter(theme)
         # logger.addHandler(console_hdlr)

         # send debug info to logfile in full details
         if logfile is None:
             logfile = "/dev/null"
         else:
             dirname = os.path.dirname(logfile)
             if dirname and not os.path.exists(dirname):
                 os.makedirs(dirname)
         # use delay to avoid empty log file
         # TODO: delay argument requires python27
         # log_hdlr = logging.FileHandler(logfile, mode="w", delay=True)
         log_hdlr = logging.FileHandler(logfile, mode="w")
         log_hdlr.setLevel(logging.DEBUG)
         log_hdlr.setFormatter(Themes["clean2"])
         logger.addHandler(log_hdlr)

         # replace excepthook with logger
         def log_exception(exc_type, exc_value, traceback):
             logger.error("Oops! We run into troubles:\n>>>")
             logger.error(exc_value, exc_info=(exc_type, exc_value, traceback))
         sys.excepthook = log_exception

         return logger

     logger = setup_logger("test")
     logger.info("here")

#+END_SRC

** configurations
   #+name: yaml-conf
   #+begin_src yaml
     version: 1
     disable_existing_loggers: true

     root:
       level: !!python/name:logging.NOTSET
       handlers: [console, logfile]

     handlers:
         logfile:
           class: logging.FileHandler
           filename: /tmp/snapman.log
           formatter: simpleFormatter
           delay: True
           level: !!python/name:logging.NOTSET
         # direct all logging msg to stdout instead of stderr
         console:
           class: logging.StreamHandler
           stream: ext://sys.stdout
           formatter: simpleFormatter
           level: !!python/name:logging.NOTSET

     formatters:
       simpleFormatter:
         class: !!python/name:logging.Formatter
         format: '%(levelname)-05s %(funcName)s@l%(lineno)-4d %(message)s'
         datefmt: '%d/%m/%Y %H:%M:%S'
   #+end_src

** basic-logger
   #+NAME: basic-logger
   #+HEADER: :noweb yes
   #+BEGIN_SRC python :results output :session logger
     def get_logger(name=None, debug=True):
         import logging
         import logging.config
         import StringIO
         import yaml

         yaml_conf = """
     <<yaml-conf>>
     """

         config = yaml.load(StringIO.StringIO(yaml_conf))
         logging.config.dictConfig(config)

         logger = logging.getLogger(name or __name__)
         level = logging.DEBUG if debug else logging.INFO
         logger.setLevel(level)

         return logger

   #+END_SRC

   #+NAME: basic-logger-test
   #+BEGIN_SRC python :results output :noweb yes
     <<basic-logger>>

     def test_logger():
         logger.debug("msg from a function")

     print("good")
     logger = get_logger()
     logger.debug("debug here")
     logger.info("info here")
     logger.error("error test")
     test_logger()


   #+END_SRC

   #+RESULTS: basic-logger-test
   : good
   : DEBUG <module>@l51   debug here
   : INFO  <module>@l52   info here
   : ERROR <module>@l53   error test
   : DEBUG test_logger@l47   msg from a function

* 备份清理算法
** 指数衰减.
   随着时间的增加, 备份保留的意义递减. 这被称为牛顿冷却定律, 其实与玻
   尔兹曼分布一回事.
*** 函数形式

    \( f(e)=p\cdot\exp(-\frac{e}{k_{b}T}) \), 其中e是变量

   积分后: \( -k_{b}T\cdot f(e) \)

   #+name: r-decay-plot
   #+begin_src R :results output graphics :file images/decay.png
     curve(exp(-0.5*x), ylim=c(-0.2,1), xlim=c(0, 10), col="blue")
     curve(exp(-0.9*x), add=T, ylim=c(-0.2, 1), xlim=c(0, 10), col="black")
     #curve((10-x)/10*exp(-1*x), add=T, ylim=c(-0.2, 1), xlim=c(0, 10), col="red")

   #+end_src

   #+RESULTS: r-decay-plot
   [[file:images/decay.png]]


*** evaluate-decay
    #+NAME: evaluate-decay
    #+BEGIN_SRC python :results output :session test :cache no
      import math

      def evaluate_decay_score(elapsed, kbt=600*0.083145):
          """ use Boltzmann distribution instead """

          assert elapsed >= 0 and kbt > 0

          score = math.exp(-1.0*elapsed / kbt)
          return score

      def evaluate_decay_area(point_from, point_to, kbt=600*0.083145, max_p=400):
          """ the Boltzmann distribution area (integration) """

          if point_from > point_to:
              point_from, point_to = point_to, point_from

          pfactor1 = 1 or (point_from - max_p + kbt)
          pfactor2 = 1 or (point_to - max_p + kbt)
          s1 = pfactor1*math.exp(-1.0*point_from / kbt)
          s2 = pfactor2*math.exp(-1.0*point_to / kbt)
          return -1*(s2 - s1)*kbt
    #+END_SRC

    #+RESULTS: evaluate-decay

    测试下看:
    #+BEGIN_SRC python :results output :cache no :noweb eval
      <<evaluate-decay>>

      kbt = 50
      p = 30
      max_period = kbt*math.log(kbt*p)
      # max_snapshots = kbt*kbt*math.exp(-1.0*max_p/kbt) - kbt*(kbt - max_p)
      print("max period: {:.1f}".format(max_period))
      snapshots_in_period1 = kbt*p*(1 - math.exp(-1.0/kbt))
      print("snapshots in p1: {:.1f}".format(snapshots_in_period1))
      print(evaluate_decay_area(109, 110, kbt)*p)
    #+END_SRC

    #+RESULTS:
    : max period: 365.7
    : snapshots in p1: 29.7
    : 3.35755841717

    如选kbt=30, p=20, 最近的一个小时可保留约20个版本, 一天(24h)中可保
    留约330个备份, 余下的分布在约8天的序列里.

** 备份清理
   1. 各个备份, 按时间序, 组成一列.
      : dir1, dir2, dir3, ..., dir9
   2. 从序列中最近的开始, 向后选择一个, 二者做比较.
      : dir1 vs dir2
      : dir1 vs dir3
   3. 通过积分公式, 容易算出这两个备份所组成的时间区域容许的最大备份数
      目alowed_snapshots.
   4. 如果allowed_snapshots < 1, 选择下一个备份, 继续2和3步. 直到最大
      两个备份的时间间隔足够的长, 满足指数衰减的要求. 然后循环下去, 构
      建一个稀疏些的备份序列.
      : dir1, dir4, dir9, ...
   5. 清除非保留队列里的备份. 任务完成.
*** tests
    #+BEGIN_SRC python :noweb eval
      <<imports>>

      <<trim-snapshots>>

      for adir, bdir in zip(adirs, bdirs):
          score = calc_snapshot_score(adir, bdir)
          #print("{}/{}: {:.2f}".format(adir, bdir, score))
          choices.append((adir, bdir))
          weights.append(score)

      ichoices = iter(choices)
      for c in bichoose(ichoices):
          print(c)

    #+END_SRC

** 权重选择
*** 基于权重选择单一项(赌轮盘算法)

    #+NAME: weighted-choose-one
    #+BEGIN_SRC python :session test :results output
      import random
      import bisect
      import operator

      def accumulate(iterable, func=operator.add):
          it = iter(iterable)
          total = next(it)
          yield total
          for element in it:
              total = func(total, element)
              yield total

      def weighted_choose_one(choices, weights):
          """ select single one based on their weights """

          assert len(choices) == len(weights)
          cumdist = list(accumulate(weights))
          x = random.random() * cumdist[-1]
          return bisect.bisect(cumdist, x)

    #+END_SRC

    #+RESULTS: weighted-choose-one

*** 基于权重选择多个项
   #+NAME: weighted-choose
   #+BEGIN_SRC python :session test :results output :noweb yes
     <<weighted-choose-one>>

     def weighted_choose(choices, weights, ncount):
         """ choose n-choices one time according their weights """

         assert len(choices) > ncount

         chosen = []
         for it in range(ncount):
             idx = weighted_choose_one(choices, weights)
             chosen.append(choices[idx])
             del choices[idx]
             del weights[idx]
         return sorted(chosen)

   #+END_SRC

   #+RESULTS: weighted-choose

*** 测试
    #+name: test-weighted-choose
    #+begin_src python :noweb eval :results output
      <<weighted-choose>>

      for c in weighted_choose(["a", "b"], [8.1, 9.9], 1):
          print(c)
    #+end_src

    #+RESULTS: test-weighted-choose
    : b

* lsyncd
** 安装与配置
   1. aur: lsyncd
   2. 需要lua中的lfs包 lua51-filesystem
   3. 设置内核变量
      #+begin_src sh
        sudo sysctl fs.inotify.max_user_watches=81920
      #+end_src

      #+RESULTS:
      : fs.inotify.max_user_watches = 81920
      
      修改/etc/sysctl.d下的文件使设置永久生效.

** lsyncd配置文件
*** 笔记
   需要安装: lua-filesystem

   见源代码中default.lua, default.checkgauge
   #+name: lua-example
   #+begin_src lua
     --
     -- used to ensure there aren't typos in the keys
     --
     default.checkgauge = {
         action        =  true,
         checkgauge    =  true,
         collect       =  true,
         delay         =  true,
         exitcodes     =  true,
         init          =  true,
         maxDelays     =  true,
         maxProcesses  =  true,
         onAttrib      =  true,
         onCreate      =  true,
         onModify      =  true,
         onDelete      =  true,
         onStartup     =  true,
         onMove        =  true,
         prepare       =  true,
         source        =  true,
         target        =  true,
     }

   #+end_src

   1. sync函数是主要的入口
   2. 几个重要的子函数
      1. init
      2. action
      3. collect
      4. prepare
   3. 默认的事件处理函数
      1. OnStartup
      2. OnAttrib
      3. OnCreate
      4. OnDelete
      5. OnModify
      6. OnMove
   4. default.action ==> onMove, onDelete ...
*** src
    #+BEGIN_SRC lua :tangle lsyncd.lua
      -----
      -- My configuration file for lsyncd 2.1
      --

      require("lfs")

      local source_rootdir = lfs.currentdir() .. "/isync-data"
      local backup_rootdir = "/btrfs/backup"
      local snapshot_rootdir = "/btrfs/TimeMachine"


      settings {
          logfile    = "/tmp/lsyncd.log",
          statusFile = "/tmp/lsyncd.status",
          statusInterval = 1,
          nodaemon   = true,
          maxDelays = 15,
      }

      ---
      -- record file change events and make btrfs snapshots accordingly
      --
      iman = {
          ---
          -- take snapshot when max_hits reached
          max_hits = 1,

          current_hits = 0,

          ---
          -- make btrfs snapshots
          --
          take_snapshot = function(event)
             log("Normal", "taking snapshot...")
             os.execute("scripts/snapman.py")
          end,
          
          ---
          -- record hits
          --
          hit = function(event)
              iman.current_hits = iman.current_hits + 1
              log("Normal", "hitted as " .. iman.current_hits)
              ---
              -- reset hits
              --
              if iman.current_hits >= iman.max_hits then
                  iman.current_hits = 0
                  iman.take_snapshot(event)
              end
          end
      }

      local isync = default.rsync

      -- isync.checkgauge.onMove = true

      -- called when a process exited.
      -- this can be a rsync command, the startup rsync or the postcmd
      isync.collect = function(agent, exitcode)
          log("Normal", "collecting here")
          if not agent.isList and agent.etype == 'Init' then
              log("Normal", "init procedure")
          else
              log("Normal", "iman hit")
              iman.hit(agent)
          end
          --- everything else, forward to default collection handler
          return default.collect(agent,exitcode)
      end

      for afile in lfs.dir(source_rootdir) do
          if afile ~= '.' and afile ~= '..' then
              local path = source_rootdir .. "/" .. afile
              local attr = lfs.attributes(path, "mode")
              if attr == "directory" then
                  source_dir = path
                  target_dir = backup_rootdir .. "/" .. afile
                  -- log("Normal", source_dir .. " ==> " .. target_dir )
                  -- using per directory filter .rsync-filter
                  sync{isync, rsync={_extra = {"-F", "--itemize-changes"}}, source=source_dir, target=target_dir, excludeFrom = "rsync.exclude"}
              end
          end
      end

    #+END_SRC
** 启动lsyncd
   #+BEGIN_SRC sh :tangle start-lsyncd.sh :shebang #! /usr/bin/env bash :results none
     # get real script path
     script_path="$0"
     [[ -h "$script_path" ]] && script_path=$(readlink "$script_path")

     script_root=$(cd $(dirname "$script_path"); pwd)
     echo "script root is: \"$script_root\""
     cd "$script_root"

     readonly LOCKFILE_DIR=/tmp
     readonly LOCK_FD=200

     lock() {
         local prefix=$1
         local fd=${2:-$LOCK_FD}
         local lock_file=$LOCKFILE_DIR/$prefix.lock

         # create lock file
         eval "exec $fd>$lock_file"

         # acquier the lock
         flock -n $fd \
             && return 0 \
             || return 1
     }

     eexit() {
         local error_str="$@"

         echo $error_str
         exit 1
     }

     # start lsyncd with lock
     # Wait for lock on /var/lock/.myscript.exclusivelock (fd 200) for 2 seconds
     lock lsyncd.lock || eexit "Only one instance of lsyncd can run at one time."
         
     while true; do
         lsyncd lsyncd.lua
         # sleep 5
         exit 0
     done


   #+END_SRC

* snapman.py [[elisp:(org-babel-tangle)][tangle]]
** outline
   #+NAME: overview
   #+HEADER: :session test
   #+HEADER: :shebang #! /usr/bin/env python2
   #+HEADER: :results output
   #+BEGIN_SRC python :noweb yes :tangle scripts/snapman.py
     <<header>>

     <<imports>>

     <<setup-logger>>

     <<evaluate-decay>>

     <<weighted-choose>>

     <<snapshot-manager>>

     <<main>>

   #+END_SRC

** imports
#+NAME: imports
#+BEGIN_SRC python :session test
  from __future__ import division

  import os
  import sys
  import time
  import datetime
  import subprocess
  import math
  import random
  import re
  import logging

#+END_SRC

#+RESULTS: imports

** header
#+NAME: header
#+BEGIN_SRC python
  # -*- coding: utf-8 -*-
  #====================================================================#
  #   DESCRIPTION:  btrfs snapshot dwim
  #
  #       OPTIONS:  ---
  #  REQUIREMENTS:  ---
  #         NOTES:  ---
  #        AUTHOR:  Wenping Guo (ybyygu)
  #         EMAIL:  winpng@gmail.com
  #       LICENCE:  GPL version 2 or upper
  #       CREATED:  2015-04-20 17:51
  #       UPDATED:
  #====================================================================#

#+END_SRC

** setup logger
   #+NAME: setup-logger
   #+BEGIN_SRC python :noweb yes :results none
     # --------------------------------------------------------------------
     # setup global logger
     # --------------------------------------------------------------------

     <<basic-logger>>

     logger = get_logger("isync")

   #+END_SRC

** class SnapshotManager
*** UML
    #+begin_src plantuml :file test.png
      class SnapshotManager {
      + take_snapshot
      + delete_snapshot
      + trim_snapshots
      }

      take_snapshot : dir_to_snapshot
      delete_snapshot --> dir_to_snapshot
      trim_snapshot --> dir_to_snapshot
      trim_snapshots --> delete_snapshot
      delete_snapshot --> _delete_snapshot



    #+end_src

    #+RESULTS:
    [[file:test.png]]

*** outline
#+NAME: snapshot-manager
#+BEGIN_SRC python :results output :session test :noweb yes
  # --------------------------------------------------------------------
  # SnapshotManager class
  # --------------------------------------------------------------------

  class SnapshotManager(object):
      def __init__(self, target_subvolume, snapshot_rootdir):
          self.target_subvolume = target_subvolume
          self.snapshot_rootdir = snapshot_rootdir
          self.decay_kbt = 30
          self.decay_pfactor = 20

          logger.info("isnapman started at {}".format(time.strftime("%Y-%m-%d %H:%M")))

      <<take-snapshot>>

      <<delete-snapshot>>

      <<trim-snapshots>>
#+END_SRC

#+RESULTS: snapshot-manager

*** take snapshot
    BTRFS 快照.
    1. 根据当前的时间, 确定快照保存的目录名: snapshot_dirname.
    2. 如果同名的快照已存在, 则不操作.

    #+NAME: take-snapshot
    #+BEGIN_SRC python :results none
      def take_snapshot(self):
          """ take one single btrfs snapshot """

          snapshot_parent_dir = os.path.join(self.snapshot_rootdir, time.strftime("%Y-%m-%d"))

          if not os.path.exists(snapshot_parent_dir):
              os.makedirs(snapshot_parent_dir)
              logger.info("Created snapshot directory: {}".format(snapshot_parent_dir))

          ##
          # take snapshot only when there is no directory named as snap_name
          # --------------------------------------------------------------------
          dirname = "{}:{:02}".format(time.strftime("%H"), int(time.strftime("%M")))
          snap_name = os.path.join(snapshot_parent_dir, dirname)
          logger.debug("snapshot directory: {}".format(snap_name))

          if not os.path.exists(snap_name):
              args = "btrfs subvolume snapshot -r".split()
              args.append(self.target_subvolume)
              args.append(snap_name)
              logger.info("Called with: {}".format(" ".join(args)))
              try:
                  subprocess.check_call(args)
              except subprocess.CalledProcessError:
                  logger.exception("Failed to take snapshot with btrfs!")
                  raise
          else:
              logger.debug("Delayed for {}".format(snap_name))

    #+END_SRC

*** delete snapshot
    #+NAME: delete-snapshot
    #+BEGIN_SRC python :results none
      def _delete_snapshot_by_path(self, snapshot_path):
          """ delete one single snapshot specified by snapshot_path """

          if not os.path.exists(snapshot_path):
              msg ="snapshot directory {} does not exists!".format(snapshot_path)
              logger.error(msg)
              raise RuntimeError(msg)

          args = "sudo btrfs subvolume delete".split()
          args.append(snapshot_path)
          logger.debug("cmdline: {}".format(" ".join(args)))
          try:
              subprocess.check_call(args)
          except subprocess.CalledProcessError:
              logger.exception("Failed to delete btrfs snapshot!")
              raise

      def delete_snapshot(self, snapshot):
          """ delete one single snapshot """

          if type(snapshot) not in (list, tuple):
              logger.error("wrong argument specified: {}".format(snapshot))
              return

          assert len(snapshot) == 2
          day_dir, time_dir = snapshot

          snapshot_path = os.path.join(self.snapshot_rootdir, day_dir, time_dir)
          logger.info("try to delete: {}".format(snapshot_path))
          self._delete_snapshot_by_path(snapshot_path)

          # remove empty directory by a day
          adir = os.path.join(self.snapshot_rootdir, day_dir)
          if not os.listdir(adir):
              os.rmdir(adir)
              logger.info("removed empty directory {}.".format(adir))

      def delete_all_snapshots(self):
          snapshots = self._get_snapshots()
          if len(snapshots) == 0:
              logger.info("No snapshots found in {}".format(self.snapshot_rootdir))
              return

          logger.info("try to delete all snapshots in {}".format(self.snapshot_rootdir))
          for s in snapshots:
              self.delete_snapshot(s)

    #+END_SRC

*** trim-snapshots
    #+NAME: trim-snapshots
    #+begin_src python
      def _snapshot_dir_to_datetime(self, day_dir, time_dir):
          return datetime.datetime.strptime("{}/{}".format(day_dir, time_dir), "%Y-%m-%d/%H:%M")

      def _calc_allowed_snapshots(self, choices):
          """ calculate how many snapshots allowed to keep """

          assert len(choices) >= 2
          first, last = choices[0], choices[-1]

          now = datetime.datetime.now()
          d1 = (now - self._snapshot_dir_to_datetime(*first)).total_seconds() / 3600
          d2 = (now - self._snapshot_dir_to_datetime(*last)).total_seconds() / 3600
          logger.debug("{:.1f} {:.1f}".format(d1, d2))

          return evaluate_decay_area(d1, d2, self.decay_kbt)*self.decay_pfactor

      def _calc_snapshot_score(self, snapshot):
          """ calculate the decay score based on its time-stamp """

          now = datetime.datetime.now()
          d1 = (now - self._snapshot_dir_to_datetime(*snapshot)).total_seconds() / 3600

          return evaluate_decay_score(d1, self.decay_kbt)

      def _get_snapshots(self):
          """ get all available snapshots on the disk """

          snapshots = []
          for adir in os.listdir(self.snapshot_rootdir):
              snap_parentdir_path = os.path.join(self.snapshot_rootdir, adir)
              for bdir in os.listdir(snap_parentdir_path):
                  try:
                      before = self._snapshot_dir_to_datetime(adir, bdir)
                      snapshots.append((adir, bdir))
                  except ValueError:
                      logger.error("Failed to parse time stamp: {}/{}".format(adir, bdir))
                      continue

          return snapshots

      def _cluster(self, ichoices, cluster=[]):

          # construct current choice and the cluster
          if cluster:
              choice1 = cluster[-1]
          else:
              choice1 = next(ichoices)
              cluster.append(choice1)

          choice2 = None
          for choice2 in ichoices:
              allowed = self._calc_allowed_snapshots([choice1, choice2])
              logger.debug("{} {} {:.1f}".format(choice1[-1], choice2[-1], allowed))
              if allowed >= 1:
                  #logger.debug("here1")
                  yield cluster
                  break
              cluster.append(choice2)
          else:
              #logger.debug("here2")
              if choice2:
                  yield cluster

          if choice2:
              #logger.debug("here3")
              for c in self._cluster(ichoices, cluster=[choice2]):
                  yield c
          else:
              logger.debug("clustering finished.")

      def _trim_snapshots_in_cluster(self, cluster, debug=False):
          """ a group of snapshots which are close in time range """

          n_snapshots = len(cluster)

          assert n_snapshots > 1

          choice1, choice2 = cluster[0], cluster[-1]
          n_allowed = self._calc_allowed_snapshots([choice1, choice2])

          n_to_delete = int(n_snapshots - n_allowed)

          if n_to_delete > 0:
              logger.info("will remove {} snapshots".format(n_to_delete))
              choices = cluster
              weights = []
              for c in choices:
                  score = self._calc_snapshot_score(c)
                  weights.append(score)

              # reverse the weights
              m = max(weights)
              for i, w in enumerate(weights):
                  weights[i] = m - w

              for adir, bdir in weighted_choose(choices, weights, n_to_delete):
                  if not debug:
                      self.delete_snapshot((adir, bdir))
                  else:
                      logger.debug("will remove {}/{}".format(adir, bdir))
          else:
              logger.error("not possible to trim this cluser. {:.0f} > {:.0f}".format(n_snapshots, n_allowed))
              logger.error("{}".format(cluster))

      def trim_snapshots(self, snapshots=[], debug=False):
          """ trim outdated snapshots with some intelligence """

          # optional argument: to make it easy for testing
          snapshots = snapshots or self._get_snapshots()
          snapshots.sort(reverse=True)

          if len(snapshots) <= 3:
              logger.info("no enough snapshots to act on.")
              return

          logger.info("Loaded {} snapshots.".format(len(snapshots)))

          ##
          # trim snapshots in a smart way
          # --------------------------------------------------------------------
          ichoices = iter(snapshots)
          snapshots_all = set(snapshots)
          snapshots_to_keep = set([snapshots[-1]])

          assert len(snapshots_all) == len(snapshots)

          for c in self._cluster(ichoices):
              for adir, bdir in c:
                  logger.debug("\t<{}/{}>".format(adir, bdir))
              if len(c) > 1:
                  self._trim_snapshots_in_cluster(c, debug)

          return 0

   #+end_src

    #+RESULTS: trim-snapshots

*** tests
     - Note taken on [2015-04-21 Tue 15:45] \\
       tests怎么加才合理?

     #+HEADER: :var adirs=tbl-dirs[,0]
     #+HEADER: :var bdirs=tbl-dirs[,1]
     #+BEGIN_SRC python :noweb yes :results output :dir scripts/
       from snapman import SnapshotManager
       <<evaluate-decay>>

       snapshots = []
       for adir, bdir in zip(adirs, bdirs):
           snapshots.append((adir, bdir))

       snapman = SnapshotManager("/btrfs/backup", "/btrfs/TimeMachine")
       snapman.decay_kbt = 30
       snapman.decay_pfactor = 60
       #snapman.take_snapshot()
       #snapman.delete_snapshot(("2015-04-22", "16:09"))
       snapman.delete_all_snapshots()
       #snapman.trim_snapshots(snapshots)
       #print(evaluate_decay_area(118, 129, 30)*30)
       #snapman.trim_snapshots(snapshots, debug=True)
       #snapman.trim_snapshots(debug=False)
     #+END_SRC

     #+RESULTS:
     #+begin_example
     INFO  __init__@l150  isnapman started at 2015-04-26 20:29
     INFO  delete_all_snapshots@l225  try to delete all snapshots in /btrfs/TimeMachine
     INFO  delete_snapshot@l210  try to delete: /btrfs/TimeMachine/2015-04-26/20:21
     DEBUG _delete_snapshot_by_path@l192  cmdline: sudo btrfs subvolume delete /btrfs/TimeMachine/2015-04-26/20:21
     Delete subvolume (no-commit): '/btrfs/TimeMachine/2015-04-26/20:21'
     INFO  delete_snapshot@l210  try to delete: /btrfs/TimeMachine/2015-04-26/20:23
     DEBUG _delete_snapshot_by_path@l192  cmdline: sudo btrfs subvolume delete /btrfs/TimeMachine/2015-04-26/20:23
     Delete subvolume (no-commit): '/btrfs/TimeMachine/2015-04-26/20:23'
     INFO  delete_snapshot@l210  try to delete: /btrfs/TimeMachine/2015-04-26/20:27
     DEBUG _delete_snapshot_by_path@l192  cmdline: sudo btrfs subvolume delete /btrfs/TimeMachine/2015-04-26/20:27
     Delete subvolume (no-commit): '/btrfs/TimeMachine/2015-04-26/20:27'
     INFO  delete_snapshot@l217  removed empty directory /btrfs/TimeMachine/2015-04-26.
#+end_example

     #+tblname: tbl-dirs
     |      adirs | bdirs |
     |------------+-------|
     | 2015-04-26 | 18:38 |
     | 2015-04-26 | 18:37 |
     | 2015-04-26 | 18:36 |
     | 2015-04-25 | 20:05 |
     | 2015-03-13 | 12:04 |
     | 2015-03-11 | 12:03 |
     | 2015-03-10 | 12:02 |

** main function
   #+NAME: main
   #+BEGIN_SRC python :results output :noweb yes
     def take_snapshot_dwim():
         snapman = SnapshotManager("/btrfs/backup", "/btrfs/TimeMachine")
         snapman.decay_kbt = 50
         snapman.decay_pfactor = 30
         snapman.take_snapshot()
         snapman.trim_snapshots()

     def main(argv=None):
         import optparse

         if argv == None: argv = sys.argv

         # parsing cmdline
         cmdl_usage = 'usage: %prog [options]...'
         cmdl_version = "%prog <<VERSION>>;" + " <<UPDATED>>"
         cmdl_parser = optparse.OptionParser(usage=cmdl_usage, \
                                             version=cmdl_version, \
                                                 conflict_handler='resolve')
         cmdl_parser.add_option('-h', '--help',
                                 action='help',
                                 help='print this help text and exit')
         cmdl_parser.add_option('-v', '--version',
                                 action='version',
                                 help='print program version and exit')
         (cmdl_opts, cmdl_args) = cmdl_parser.parse_args()

         take_snapshot_dwim()
         return

     if __name__ == '__main__':
         main()

   #+END_SRC

   #+RESULTS: main-function

* Local variables
# Local Variables:
# time-stamp-format: "%:y-%02m-%02d %02H:%02M"
# time-stamp-pattern: "100/^Updated: %%$"
# End:
